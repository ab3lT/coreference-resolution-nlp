{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35263e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Set\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e055a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 1. GOLD STANDARD ANNOTATIONS\n",
    "# ============================================================================\n",
    "\n",
    "GOLD_ANNOTATIONS = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"text\": \"Sarah bought a new car. She drove it to work. The vehicle performed excellently.\",\n",
    "        \"entities\": [\n",
    "            {\"entity_id\": 1, \"mentions\": [\"Sarah\", \"She\"]},\n",
    "            {\"entity_id\": 2, \"mentions\": [\"a new car\", \"it\", \"The vehicle\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"text\": \"John met Mary at the coffee shop. He ordered tea while she got a cappuccino. Both enjoyed their drinks.\",\n",
    "        \"entities\": [\n",
    "            {\"entity_id\": 1, \"mentions\": [\"John\", \"He\"]},\n",
    "            {\"entity_id\": 2, \"mentions\": [\"Mary\", \"she\"]},\n",
    "            {\"entity_id\": 3, \"mentions\": [\"the coffee shop\"]},\n",
    "            {\"entity_id\": 4, \"mentions\": [\"tea\"]},\n",
    "            {\"entity_id\": 5, \"mentions\": [\"a cappuccino\"]},\n",
    "            {\"entity_id\": 6, \"mentions\": [\"Both\", \"their drinks\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"text\": \"Apple announced its new iPhone. The device features an advanced camera. Tim Cook presented it at the conference.\",\n",
    "        \"entities\": [\n",
    "            {\"entity_id\": 1, \"mentions\": [\"Apple\"]},\n",
    "            {\"entity_id\": 2, \"mentions\": [\"its new iPhone\", \"The device\", \"it\"]},\n",
    "            {\"entity_id\": 3, \"mentions\": [\"Tim Cook\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"text\": \"The lawyer called the client after he finished the report. It was crucial for the case.\",\n",
    "        \"entities\": [\n",
    "            {\"entity_id\": 1, \"mentions\": [\"The lawyer\", \"he\"]},\n",
    "            {\"entity_id\": 2, \"mentions\": [\"the client\"]},\n",
    "            {\"entity_id\": 3, \"mentions\": [\"the report\", \"It\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"text\": \"Emma and her brother visited the museum. They spent hours looking at paintings. The artworks fascinated them.\",\n",
    "        \"entities\": [\n",
    "            {\"entity_id\": 1, \"mentions\": [\"Emma\"]},\n",
    "            {\"entity_id\": 2, \"mentions\": [\"her brother\"]},\n",
    "            {\"entity_id\": 3, \"mentions\": [\"the museum\"]},\n",
    "            {\"entity_id\": 4, \"mentions\": [\"They\", \"them\"]},\n",
    "            {\"entity_id\": 5, \"mentions\": [\"paintings\", \"The artworks\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"text\": \"The company released a new product. It was designed for mobile users. Customers loved the innovation.\",\n",
    "        \"entities\": [\n",
    "            {\"entity_id\": 1, \"mentions\": [\"The company\"]},\n",
    "            {\"entity_id\": 2, \"mentions\": [\"a new product\", \"It\"]},\n",
    "            {\"entity_id\": 3, \"mentions\": [\"mobile users\"]},\n",
    "            {\"entity_id\": 4, \"mentions\": [\"Customers\"]},\n",
    "            {\"entity_id\": 5, \"mentions\": [\"the innovation\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"text\": \"Peter gave his book to Michael. He read it quickly. The story impressed him.\",\n",
    "        \"entities\": [\n",
    "            {\"entity_id\": 1, \"mentions\": [\"Peter\", \"He\"]},\n",
    "            {\"entity_id\": 2, \"mentions\": [\"his book\", \"it\", \"The story\"]},\n",
    "            {\"entity_id\": 3, \"mentions\": [\"Michael\", \"him\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"text\": \"The hospital admitted a patient yesterday. She required immediate treatment. The doctors examined her carefully.\",\n",
    "        \"entities\": [\n",
    "            {\"entity_id\": 1, \"mentions\": [\"The hospital\"]},\n",
    "            {\"entity_id\": 2, \"mentions\": [\"a patient\", \"She\", \"her\"]},\n",
    "            {\"entity_id\": 3, \"mentions\": [\"The doctors\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 9,\n",
    "        \"text\": \"Google acquired a startup. Its founders were thrilled. They received stock options and cash.\",\n",
    "        \"entities\": [\n",
    "            {\"entity_id\": 1, \"mentions\": [\"Google\"]},\n",
    "            {\"entity_id\": 2, \"mentions\": [\"a startup\"]},\n",
    "            {\"entity_id\": 3, \"mentions\": [\"Its founders\", \"They\"]},\n",
    "            {\"entity_id\": 4, \"mentions\": [\"stock options and cash\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"text\": \"The professor explained the theory. Students listened attentively. It was complex but fascinating.\",\n",
    "        \"entities\": [\n",
    "            {\"entity_id\": 1, \"mentions\": [\"The professor\"]},\n",
    "            {\"entity_id\": 2, \"mentions\": [\"the theory\", \"It\"]},\n",
    "            {\"entity_id\": 3, \"mentions\": [\"Students\"]}\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f5bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CoreferenceResolver:\n",
    "    \"\"\"\n",
    "    Rule-based coreference resolution system.\n",
    "    For demonstration, uses heuristics combined with mention extraction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pronouns = {\n",
    "            'he': 'male', 'him': 'male', 'his': 'male',\n",
    "            'she': 'female', 'her': 'female', 'hers': 'female',\n",
    "            'it': 'neuter', 'its': 'neuter',\n",
    "            'they': 'plural', 'them': 'plural', 'their': 'plural',\n",
    "            'both': 'plural'\n",
    "        }\n",
    "    \n",
    "    def extract_mentions(self, text: str) -> List[Tuple[str, int, int]]:\n",
    "        \"\"\"Extract mention candidates (nouns, pronouns, proper nouns).\"\"\"\n",
    "        mentions = []\n",
    "        \n",
    "        # Extract pronouns\n",
    "        for pronoun in self.pronouns.keys():\n",
    "            pattern = r'\\b' + pronoun + r'\\b'\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                mentions.append((pronoun.lower(), match.start(), match.end()))\n",
    "        \n",
    "        # Extract noun phrases (simplified)\n",
    "        noun_patterns = [\n",
    "            r'\\b[A-Z][a-z]+\\b',  # Proper nouns\n",
    "            r'\\bthe\\s+(?:[\\w]+\\s+)*[\\w]+\\b',  # Definite descriptions\n",
    "            r'\\ba\\s+(?:[\\w]+\\s+)*[\\w]+\\b',  # Indefinite descriptions\n",
    "        ]\n",
    "        \n",
    "        for pattern in noun_patterns:\n",
    "            for match in re.finditer(pattern, text):\n",
    "                mentions.append((match.group(), match.start(), match.end()))\n",
    "        \n",
    "        return mentions\n",
    "    \n",
    "    def resolve(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Resolve coreferences using simple heuristics.\"\"\"\n",
    "        mentions = self.extract_mentions(text)\n",
    "        chains = []\n",
    "        processed = set()\n",
    "        \n",
    "        for i, (mention, start, end) in enumerate(mentions):\n",
    "            if i in processed:\n",
    "                continue\n",
    "            \n",
    "            chain = [mention]\n",
    "            processed.add(i)\n",
    "            \n",
    "            # Find matching pronouns or similar mentions\n",
    "            for j, (other_mention, other_start, other_end) in enumerate(mentions[i+1:], start=i+1):\n",
    "                if j in processed:\n",
    "                    continue\n",
    "                \n",
    "                # Simple matching heuristic\n",
    "                if self._should_link(mention, other_mention):\n",
    "                    chain.append(other_mention)\n",
    "                    processed.add(j)\n",
    "            \n",
    "            if len(chain) > 0:\n",
    "                chains.append({\n",
    "                    \"entity_id\": len(chains) + 1,\n",
    "                    \"mentions\": chain\n",
    "                })\n",
    "        \n",
    "        return chains\n",
    "    \n",
    "    def _should_link(self, mention1: str, mention2: str) -> bool:\n",
    "        \"\"\"Determine if two mentions should be linked.\"\"\"\n",
    "        m1_lower = mention1.lower()\n",
    "        m2_lower = mention2.lower()\n",
    "        \n",
    "        # Same mention\n",
    "        if m1_lower == m2_lower:\n",
    "            return True\n",
    "        \n",
    "        # Pronoun resolution heuristic\n",
    "        if m2_lower in self.pronouns:\n",
    "            return False  # Simplified: skip complex pronoun resolution\n",
    "        \n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a10be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 3. EVALUATION METRICS: MUC\n",
    "# ============================================================================\n",
    "\n",
    "class MUCMetric:\n",
    "    \"\"\"\n",
    "    Muelas Unified Clustering (MUC) Metric\n",
    "    Link-based evaluation: counts links formed/missing\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_links(chains: List[Dict]) -> Set[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Extract all links from coreference chains.\n",
    "        A link is a pair of mentions in the same chain.\n",
    "        \"\"\"\n",
    "        links = set()\n",
    "        for chain in chains:\n",
    "            mentions = chain.get(\"mentions\", [])\n",
    "            if len(mentions) < 2:\n",
    "                continue\n",
    "            for i in range(len(mentions) - 1):\n",
    "                link = tuple(sorted([i, i + 1]))\n",
    "                links.add(link)\n",
    "        return links\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute(gold_chains: List[Dict], system_chains: List[Dict]) -> Dict:\n",
    "        \"\"\"Compute MUC precision, recall, and F1.\"\"\"\n",
    "        gold_links = MUCMetric.get_links(gold_chains)\n",
    "        system_links = MUCMetric.get_links(system_chains)\n",
    "        \n",
    "        if len(system_links) == 0:\n",
    "            precision = 0.0\n",
    "        else:\n",
    "            precision = len(gold_links & system_links) / len(system_links)\n",
    "        \n",
    "        if len(gold_links) == 0:\n",
    "            recall = 1.0 if len(system_links) == 0 else 0.0\n",
    "        else:\n",
    "            recall = len(gold_links & system_links) / len(gold_links)\n",
    "        \n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            \"precision\": round(precision, 4),\n",
    "            \"recall\": round(recall, 4),\n",
    "            \"f1\": round(f1, 4)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f2a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 4. EVALUATION METRICS: CEAF\n",
    "# ============================================================================\n",
    "\n",
    "class CEAFMetric:\n",
    "    \"\"\"\n",
    "    CEAF-e (Entity-based CEAF)\n",
    "    Optimal alignment between system and gold entities using mention overlap\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def mention_overlap(gold_mentions: List[str], system_mentions: List[str]) -> float:\n",
    "        \"\"\"Compute similarity as number of overlapping mentions.\"\"\"\n",
    "        gold_set = set(m.lower() for m in gold_mentions)\n",
    "        system_set = set(m.lower() for m in system_mentions)\n",
    "        overlap = len(gold_set & system_set)\n",
    "        return overlap\n",
    "    \n",
    "    @staticmethod\n",
    "    def optimal_alignment(gold_chains: List[Dict], system_chains: List[Dict]) -> Tuple[float, List]:\n",
    "        \"\"\"\n",
    "        Find optimal alignment using greedy matching.\n",
    "        Returns (total_similarity, alignment_list)\n",
    "        \"\"\"\n",
    "        if not gold_chains or not system_chains:\n",
    "            return 0.0, []\n",
    "        \n",
    "        gold_entities = [(i, chain[\"mentions\"]) for i, chain in enumerate(gold_chains)]\n",
    "        system_entities = [(i, chain[\"mentions\"]) for i, chain in enumerate(system_chains)]\n",
    "        \n",
    "        matched_gold = set()\n",
    "        matched_system = set()\n",
    "        alignment = []\n",
    "        total_similarity = 0.0\n",
    "        \n",
    "        # Greedy matching: highest overlap first\n",
    "        similarities = []\n",
    "        for g_idx, (g_id, g_mentions) in enumerate(gold_entities):\n",
    "            for s_idx, (s_id, s_mentions) in enumerate(system_entities):\n",
    "                sim = CEAFMetric.mention_overlap(g_mentions, s_mentions)\n",
    "                similarities.append((sim, g_idx, s_idx))\n",
    "        \n",
    "        similarities.sort(reverse=True)\n",
    "        \n",
    "        for sim, g_idx, s_idx in similarities:\n",
    "            if g_idx not in matched_gold and s_idx not in matched_system and sim > 0:\n",
    "                matched_gold.add(g_idx)\n",
    "                matched_system.add(s_idx)\n",
    "                alignment.append((g_idx, s_idx, sim))\n",
    "                total_similarity += sim\n",
    "        \n",
    "        return total_similarity, alignment\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute(gold_chains: List[Dict], system_chains: List[Dict]) -> Dict:\n",
    "        \"\"\"Compute CEAF-e precision, recall, and F1.\"\"\"\n",
    "        total_sim, _ = CEAFMetric.optimal_alignment(gold_chains, system_chains)\n",
    "        \n",
    "        # Precision: total similarity / sum of system entity sizes\n",
    "        system_size = sum(len(chain[\"mentions\"]) for chain in system_chains)\n",
    "        precision = total_sim / system_size if system_size > 0 else 0.0\n",
    "        \n",
    "        # Recall: total similarity / sum of gold entity sizes\n",
    "        gold_size = sum(len(chain[\"mentions\"]) for chain in gold_chains)\n",
    "        recall = total_sim / gold_size if gold_size > 0 else 0.0\n",
    "        \n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            \"precision\": round(precision, 4),\n",
    "            \"recall\": round(recall, 4),\n",
    "            \"f1\": round(f1, 4)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d00cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 5. EVALUATION RUNNER\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_system(gold_annotations: List[Dict]) -> Dict:\n",
    "    \"\"\"Evaluate the coreference resolver on gold annotations.\"\"\"\n",
    "    resolver = CoreferenceResolver()\n",
    "    \n",
    "    all_muc_scores = {\"precision\": [], \"recall\": [], \"f1\": []}\n",
    "    all_ceaf_scores = {\"precision\": [], \"recall\": [], \"f1\": []}\n",
    "    \n",
    "    results_per_document = []\n",
    "    \n",
    "    for doc in gold_annotations:\n",
    "        text = doc[\"text\"]\n",
    "        gold_chains = doc[\"entities\"]\n",
    "        system_chains = resolver.resolve(text)\n",
    "        \n",
    "        muc_result = MUCMetric.compute(gold_chains, system_chains)\n",
    "        ceaf_result = CEAFMetric.compute(gold_chains, system_chains)\n",
    "        \n",
    "        for key in all_muc_scores:\n",
    "            all_muc_scores[key].append(muc_result[key])\n",
    "        for key in all_ceaf_scores:\n",
    "            all_ceaf_scores[key].append(ceaf_result[key])\n",
    "        \n",
    "        results_per_document.append({\n",
    "            \"doc_id\": doc[\"id\"],\n",
    "            \"text\": text,\n",
    "            \"gold\": gold_chains,\n",
    "            \"system\": system_chains,\n",
    "            \"muc\": muc_result,\n",
    "            \"ceaf\": ceaf_result\n",
    "        })\n",
    "    \n",
    "    # Compute macro averages\n",
    "    avg_muc = {k: round(sum(v) / len(v), 4) for k, v in all_muc_scores.items()}\n",
    "    avg_ceaf = {k: round(sum(v) / len(v), 4) for k, v in all_ceaf_scores.items()}\n",
    "    \n",
    "    return {\n",
    "        \"per_document\": results_per_document,\n",
    "        \"muc_average\": avg_muc,\n",
    "        \"ceaf_average\": avg_ceaf\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ece96e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 6. VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_html_visualization(text: str, chains: List[Dict]) -> str:\n",
    "    \"\"\"Generate HTML visualization with color-coded entities.\"\"\"\n",
    "    colors = [\n",
    "        \"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\", \"#FFA07A\", \"#98D8C8\",\n",
    "        \"#F7DC6F\", \"#BB8FCE\", \"#85C1E2\", \"#F8B88B\", \"#ABEBC6\"\n",
    "    ]\n",
    "    \n",
    "    # Create mention to entity mapping\n",
    "    mention_to_entity = {}\n",
    "    for chain in chains:\n",
    "        entity_id = chain[\"entity_id\"]\n",
    "        for mention in chain[\"mentions\"]:\n",
    "            mention_to_entity[mention.lower()] = entity_id\n",
    "    \n",
    "    # Tokenize and colorize\n",
    "    words = text.split()\n",
    "    colored_html = '<div style=\"font-size: 16px; line-height: 1.8; font-family: Arial;\">'\n",
    "    \n",
    "    for word in words:\n",
    "        word_clean = word.rstrip('.,;:!?')\n",
    "        punctuation = word[len(word_clean):]\n",
    "        \n",
    "        if word_clean.lower() in mention_to_entity:\n",
    "            entity_id = mention_to_entity[word_clean.lower()]\n",
    "            color = colors[(entity_id - 1) % len(colors)]\n",
    "            colored_html += f'<span style=\"background-color: {color}; padding: 2px 4px; margin: 0 2px;\">{word_clean}</span>{punctuation} '\n",
    "        else:\n",
    "            colored_html += word + ' '\n",
    "    \n",
    "    colored_html += '</div>'\n",
    "    return colored_html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd9edaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COREFERENCE RESOLUTION SYSTEM - EVALUATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "OVERALL METRICS (Macro-Averaged)\n",
      "================================================================================\n",
      "\n",
      "Metric     | Precision    | Recall       | F1          \n",
      "--------------------------------------------------\n",
      "MUC        | 0.9          | 0.75         | 0.8         \n",
      "CEAF       | 0.3823       | 0.4413       | 0.4087      \n",
      "\n",
      "================================================================================\n",
      "SAMPLE DETAILED RESULTS (First 3 Documents)\n",
      "================================================================================\n",
      "\n",
      "Document 1: Sarah bought a new car. She drove it to work. The vehicle pe...\n",
      "  Gold Entities: [{'entity_id': 1, 'mentions': ['Sarah', 'She']}, {'entity_id': 2, 'mentions': ['a new car', 'it', 'The vehicle']}]\n",
      "  System Entities: [{'entity_id': 1, 'mentions': ['she', 'She']}, {'entity_id': 2, 'mentions': ['it']}, {'entity_id': 3, 'mentions': ['Sarah']}, {'entity_id': 4, 'mentions': ['The']}, {'entity_id': 5, 'mentions': ['a new car']}]\n",
      "  MUC: P=1.0, R=0.5, F1=0.6667\n",
      "  CEAF: P=0.3333, R=0.4, F1=0.3636\n",
      "\n",
      "Document 2: John met Mary at the coffee shop. He ordered tea while she g...\n",
      "  Gold Entities: [{'entity_id': 1, 'mentions': ['John', 'He']}, {'entity_id': 2, 'mentions': ['Mary', 'she']}, {'entity_id': 3, 'mentions': ['the coffee shop']}, {'entity_id': 4, 'mentions': ['tea']}, {'entity_id': 5, 'mentions': ['a cappuccino']}, {'entity_id': 6, 'mentions': ['Both', 'their drinks']}]\n",
      "  System Entities: [{'entity_id': 1, 'mentions': ['he', 'He']}, {'entity_id': 2, 'mentions': ['she']}, {'entity_id': 3, 'mentions': ['their']}, {'entity_id': 4, 'mentions': ['both', 'Both']}, {'entity_id': 5, 'mentions': ['John']}, {'entity_id': 6, 'mentions': ['Mary']}, {'entity_id': 7, 'mentions': ['the coffee shop']}, {'entity_id': 8, 'mentions': ['a cappuccino']}]\n",
      "  MUC: P=1.0, R=1.0, F1=1.0\n",
      "  CEAF: P=0.5, R=0.5556, F1=0.5263\n",
      "\n",
      "Document 3: Apple announced its new iPhone. The device features an advan...\n",
      "  Gold Entities: [{'entity_id': 1, 'mentions': ['Apple']}, {'entity_id': 2, 'mentions': ['its new iPhone', 'The device', 'it']}, {'entity_id': 3, 'mentions': ['Tim Cook']}]\n",
      "  System Entities: [{'entity_id': 1, 'mentions': ['it']}, {'entity_id': 2, 'mentions': ['its']}, {'entity_id': 3, 'mentions': ['Apple']}, {'entity_id': 4, 'mentions': ['The']}, {'entity_id': 5, 'mentions': ['Tim']}, {'entity_id': 6, 'mentions': ['Cook']}, {'entity_id': 7, 'mentions': ['the conference']}]\n",
      "  MUC: P=0.0, R=0.0, F1=0.0\n",
      "  CEAF: P=0.2857, R=0.4, F1=0.3333\n",
      "\n",
      "================================================================================\n",
      "Results saved to: coref_evaluation_results.json\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 7. MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(\"COREFERENCE RESOLUTION SYSTEM - EVALUATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluate_system(GOLD_ANNOTATIONS)\n",
    "    \n",
    "    # Print overall metrics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OVERALL METRICS (Macro-Averaged)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n{'Metric':<10} | {'Precision':<12} | {'Recall':<12} | {'F1':<12}\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"{'MUC':<10} | {results['muc_average']['precision']:<12} | {results['muc_average']['recall']:<12} | {results['muc_average']['f1']:<12}\")\n",
    "    print(f\"{'CEAF':<10} | {results['ceaf_average']['precision']:<12} | {results['ceaf_average']['recall']:<12} | {results['ceaf_average']['f1']:<12}\")\n",
    "    \n",
    "    # Sample detailed results (first 3 documents)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE DETAILED RESULTS (First 3 Documents)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for result in results[\"per_document\"][:3]:\n",
    "        print(f\"\\nDocument {result['doc_id']}: {result['text'][:60]}...\")\n",
    "        print(f\"  Gold Entities: {result['gold']}\")\n",
    "        print(f\"  System Entities: {result['system']}\")\n",
    "        print(f\"  MUC: P={result['muc']['precision']}, R={result['muc']['recall']}, F1={result['muc']['f1']}\")\n",
    "        print(f\"  CEAF: P={result['ceaf']['precision']}, R={result['ceaf']['recall']}, F1={result['ceaf']['f1']}\")\n",
    "    \n",
    "    # Save results to JSON\n",
    "    with open(\"coref_evaluation_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Results saved to: coref_evaluation_results.json\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9572b7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED AMBIGUITY AND ERROR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "CASE: Pronoun Ambiguity\n",
      "------------------------------------------------------------\n",
      "Text: The lawyer called the client after he finished the report.\n",
      "\n",
      "Linguistic Analysis:\n",
      "\n",
      "The pronoun 'he' has two syntactically plausible antecedents:\n",
      "1. \"The lawyer\" (subject of main clause) - Preferred by Binding Theory\n",
      "2. \"the client\" (object of main clause) - Possible but less likely\n",
      "\n",
      "LINGUISTIC CUES FOR RESOLUTION:\n",
      "- Syntactic c-command: The lawyer c-commands the client\n",
      "- Recency: The client is closer, but subject has priority\n",
      "- Semantic plausibility: Who can \"finish a report\"? Both are plausible.\n",
      "- World knowledge: Typically lawyers write reports for clients.\n",
      "\n",
      "CHALLENGES FOR ML SYSTEMS:\n",
      "- Requires syntactic tree construction\n",
      "- Needs semantic understanding of predicate-argument structure\n",
      "- Depends on pragmatic/common-sense reasoning\n",
      "\n",
      "System Error: Rule-based system lacks syntactic analysis and semantic reasoning\n",
      "\n",
      "CASE: Nested Entity Ambiguity\n",
      "------------------------------------------------------------\n",
      "Text: Apple's CEO announced their new product.\n",
      "\n",
      "Linguistic Analysis:\n",
      "\n",
      "The possessive pronoun 'their' is ambiguous:\n",
      "1. Their = Apple (organization, plural interpretation)\n",
      "2. Their = Apple's CEO (person, but using plural form)\n",
      "\n",
      "LINGUISTIC CUES FOR RESOLUTION:\n",
      "- Entity type mismatch: \"their\" expects plural, CEO is singular\n",
      "- Possessive structure: \"Apple's CEO\" indicates person subsumed under org\n",
      "- Semantic selection: Products belong to organizations primarily\n",
      "- Predicate structure: \"announced\" fits both interpretations\n",
      "\n",
      "CHALLENGES FOR ML SYSTEMS:\n",
      "- Requires entity type classification\n",
      "- Needs handling of possessive attachment\n",
      "- Understanding of implicit entity relationships\n",
      "- Generic plural pronoun usage\n",
      "\n",
      "System Error: System fails to handle possessive constructions and entity types\n",
      "\n",
      "CASE: Bridging Reference\n",
      "------------------------------------------------------------\n",
      "Text: The company released a new phone. The device featured an advanced camera.\n",
      "\n",
      "Linguistic Analysis:\n",
      "\n",
      "\"The device\" is a bridging reference to \"a new phone\" - they are not identical\n",
      "mentions but related through semantic knowledge (device part of phone).\n",
      "\n",
      "LINGUISTIC CUES FOR RESOLUTION:\n",
      "- Semantic relation: phone ⊃ device (part-of relationship)\n",
      "- Discourse context: Expected to discuss phone properties\n",
      "- Lexical chains: \"phone\" and \"device\" are related\n",
      "- Article use: \"The device\" presupposes familiarity from context\n",
      "\n",
      "CHALLENGES FOR ML SYSTEMS:\n",
      "- Requires semantic/knowledge graph understanding\n",
      "- Bridging references are subtle and context-dependent\n",
      "- Not taught explicitly in many coreference datasets\n",
      "- Depends on world knowledge (what parts do phones have?)\n",
      "\n",
      "System Error: Rule-based approach treats only identical mentions as coreferent\n",
      "\n",
      "\n",
      "To use this module with your results:\n",
      "1. Load evaluation results from coref_evaluation_results.json\n",
      "2. Call generate_results_table(results) for summary\n",
      "3. Call generate_mention_error_statistics(results['per_document']) for error analysis\n",
      "4. Use TerminalVisualizer.visualize(text, chains) for color-coded output\n",
      "5. Use ErrorAnalyzer.analyze_result() for detailed per-document error analysis\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extended Analysis and Visualization Module\n",
    "Complements the main coreference resolution system\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# COLORED TERMINAL OUTPUT VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "class TerminalVisualizer:\n",
    "    \"\"\"Generate colored terminal output for coreference chains.\"\"\"\n",
    "    \n",
    "    COLORS = {\n",
    "        1: '\\033[94m',   # Blue\n",
    "        2: '\\033[92m',   # Green\n",
    "        3: '\\033[91m',   # Red\n",
    "        4: '\\033[93m',   # Yellow\n",
    "        5: '\\033[95m',   # Magenta\n",
    "        6: '\\033[96m',   # Cyan\n",
    "        7: '\\033[97m',   # White\n",
    "        8: '\\033[44m',   # Blue background\n",
    "        9: '\\033[42m',   # Green background\n",
    "        10: '\\033[41m',  # Red background\n",
    "    }\n",
    "    RESET = '\\033[0m'\n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize(text: str, chains: List[Dict]) -> str:\n",
    "        \"\"\"Generate colored terminal visualization.\"\"\"\n",
    "        # Create mention to entity mapping\n",
    "        mention_to_entity = {}\n",
    "        for chain in chains:\n",
    "            entity_id = chain[\"entity_id\"]\n",
    "            for mention in chain[\"mentions\"]:\n",
    "                mention_to_entity[mention.lower()] = entity_id\n",
    "        \n",
    "        # Process words\n",
    "        words = text.split()\n",
    "        output_lines = []\n",
    "        current_line = \"\"\n",
    "        \n",
    "        for word in words:\n",
    "            word_clean = word.rstrip('.,;:!?')\n",
    "            punctuation = word[len(word_clean):]\n",
    "            \n",
    "            # Check if word matches a mention\n",
    "            matched = False\n",
    "            for mention_key, entity_id in mention_to_entity.items():\n",
    "                if word_clean.lower() == mention_key:\n",
    "                    color = TerminalVisualizer.COLORS.get(entity_id, '\\033[94m')\n",
    "                    current_line += f\"{color}[{word_clean}]_{entity_id}{TerminalVisualizer.RESET}{punctuation} \"\n",
    "                    matched = True\n",
    "                    break\n",
    "            \n",
    "            if not matched:\n",
    "                current_line += word + \" \"\n",
    "            \n",
    "            # Line wrapping for readability\n",
    "            if len(current_line) > 80:\n",
    "                output_lines.append(current_line)\n",
    "                current_line = \"\"\n",
    "        \n",
    "        if current_line:\n",
    "            output_lines.append(current_line)\n",
    "        \n",
    "        return \"\\n\".join(output_lines)\n",
    "\n",
    "# ============================================================================\n",
    "# DETAILED ERROR ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "class ErrorAnalyzer:\n",
    "    \"\"\"Detailed analysis of coreference resolution errors.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def classify_error(gold_mention: str, system_chain: List[str], \n",
    "                      gold_chain: List[str]) -> str:\n",
    "        \"\"\"Classify the type of error made.\"\"\"\n",
    "        gold_set = set(m.lower() for m in gold_chain)\n",
    "        system_set = set(m.lower() for m in system_chain)\n",
    "        \n",
    "        if gold_set == system_set:\n",
    "            return \"CORRECT\"\n",
    "        elif gold_set.issubset(system_set):\n",
    "            return \"FALSE_POSITIVE\"  # System linked extra mentions\n",
    "        elif gold_set.issuperset(system_set):\n",
    "            return \"FALSE_NEGATIVE\"  # System missed mentions\n",
    "        else:\n",
    "            return \"PARTIAL_MATCH\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_mention_type(mention: str) -> str:\n",
    "        \"\"\"Classify mention type.\"\"\"\n",
    "        mention_lower = mention.lower()\n",
    "        \n",
    "        if mention_lower in ['he', 'she', 'it', 'they', 'him', 'her', 'them', 'both']:\n",
    "            return \"PRONOUN\"\n",
    "        elif mention[0].isupper() and len(mention.split()) == 1:\n",
    "            return \"PROPER_NOUN\"\n",
    "        elif mention.startswith('the '):\n",
    "            return \"DEFINITE_DESCRIPTION\"\n",
    "        elif mention.startswith('a '):\n",
    "            return \"INDEFINITE_DESCRIPTION\"\n",
    "        else:\n",
    "            return \"OTHER\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_result(gold_chains: List[Dict], system_chains: List[Dict], \n",
    "                      text: str) -> Dict:\n",
    "        \"\"\"Comprehensive error analysis for a single document.\"\"\"\n",
    "        \n",
    "        analysis = {\n",
    "            \"total_gold_entities\": len(gold_chains),\n",
    "            \"total_system_entities\": len(system_chains),\n",
    "            \"gold_mention_types\": {},\n",
    "            \"error_types\": {},\n",
    "            \"false_positives\": [],\n",
    "            \"false_negatives\": [],\n",
    "            \"partial_matches\": []\n",
    "        }\n",
    "        \n",
    "        # Count mention types in gold\n",
    "        for chain in gold_chains:\n",
    "            for mention in chain[\"mentions\"]:\n",
    "                m_type = ErrorAnalyzer.analyze_mention_type(mention)\n",
    "                analysis[\"gold_mention_types\"][m_type] = \\\n",
    "                    analysis[\"gold_mention_types\"].get(m_type, 0) + 1\n",
    "        \n",
    "        # Create gold mention to chain mapping\n",
    "        gold_mention_to_chain = {}\n",
    "        for chain in gold_chains:\n",
    "            for mention in chain[\"mentions\"]:\n",
    "                gold_mention_to_chain[mention.lower()] = chain[\"mentions\"]\n",
    "        \n",
    "        # Analyze system output\n",
    "        for sys_chain in system_chains:\n",
    "            sys_mentions_lower = [m.lower() for m in sys_chain[\"mentions\"]]\n",
    "            \n",
    "            # Find corresponding gold chain\n",
    "            gold_chain_match = None\n",
    "            for gold_mention in sys_mentions_lower:\n",
    "                if gold_mention in gold_mention_to_chain:\n",
    "                    gold_chain_match = gold_mention_to_chain[gold_mention]\n",
    "                    break\n",
    "            \n",
    "            if gold_chain_match is None:\n",
    "                # False positive: entire system chain is spurious\n",
    "                analysis[\"false_positives\"].append(sys_chain[\"mentions\"])\n",
    "            else:\n",
    "                # Partial match or false positive/negative\n",
    "                error_type = ErrorAnalyzer.classify_error(\n",
    "                    sys_chain[\"mentions\"][0],\n",
    "                    sys_chain[\"mentions\"],\n",
    "                    gold_chain_match\n",
    "                )\n",
    "                analysis[\"error_types\"][error_type] = \\\n",
    "                    analysis[\"error_types\"].get(error_type, 0) + 1\n",
    "                \n",
    "                if error_type == \"FALSE_POSITIVE\":\n",
    "                    analysis[\"false_positives\"].append({\n",
    "                        \"system\": sys_chain[\"mentions\"],\n",
    "                        \"gold\": gold_chain_match\n",
    "                    })\n",
    "                elif error_type == \"PARTIAL_MATCH\":\n",
    "                    analysis[\"partial_matches\"].append({\n",
    "                        \"system\": sys_chain[\"mentions\"],\n",
    "                        \"gold\": gold_chain_match\n",
    "                    })\n",
    "        \n",
    "        # Find false negatives (gold chains not covered)\n",
    "        covered_gold = set()\n",
    "        for sys_chain in system_chains:\n",
    "            for mention in sys_chain[\"mentions\"]:\n",
    "                if mention.lower() in gold_mention_to_chain:\n",
    "                    covered_gold.add(tuple(sorted([m.lower() for m in \n",
    "                                          gold_mention_to_chain[mention.lower()]])))\n",
    "        \n",
    "        for gold_chain in gold_chains:\n",
    "            gold_key = tuple(sorted([m.lower() for m in gold_chain[\"mentions\"]]))\n",
    "            if gold_key not in covered_gold:\n",
    "                analysis[\"false_negatives\"].append(gold_chain[\"mentions\"])\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# ============================================================================\n",
    "# AMBIGUITY CASE STUDY\n",
    "# ============================================================================\n",
    "\n",
    "AMBIGUITY_EXAMPLES = [\n",
    "    {\n",
    "        \"case\": \"Pronoun Ambiguity\",\n",
    "        \"text\": \"The lawyer called the client after he finished the report.\",\n",
    "        \"pronouns\": [\"he\"],\n",
    "        \"antecedents\": [\"The lawyer\", \"the client\"],\n",
    "        \"linguistic_analysis\": \"\"\"\n",
    "The pronoun 'he' has two syntactically plausible antecedents:\n",
    "1. \"The lawyer\" (subject of main clause) - Preferred by Binding Theory\n",
    "2. \"the client\" (object of main clause) - Possible but less likely\n",
    "\n",
    "LINGUISTIC CUES FOR RESOLUTION:\n",
    "- Syntactic c-command: The lawyer c-commands the client\n",
    "- Recency: The client is closer, but subject has priority\n",
    "- Semantic plausibility: Who can \"finish a report\"? Both are plausible.\n",
    "- World knowledge: Typically lawyers write reports for clients.\n",
    "\n",
    "CHALLENGES FOR ML SYSTEMS:\n",
    "- Requires syntactic tree construction\n",
    "- Needs semantic understanding of predicate-argument structure\n",
    "- Depends on pragmatic/common-sense reasoning\n",
    "\"\"\",\n",
    "        \"system_error\": \"Rule-based system lacks syntactic analysis and semantic reasoning\"\n",
    "    },\n",
    "    {\n",
    "        \"case\": \"Nested Entity Ambiguity\",\n",
    "        \"text\": \"Apple's CEO announced their new product.\",\n",
    "        \"entities\": [\"Apple\", \"Apple's CEO\"],\n",
    "        \"pronouns\": [\"their\"],\n",
    "        \"linguistic_analysis\": \"\"\"\n",
    "The possessive pronoun 'their' is ambiguous:\n",
    "1. Their = Apple (organization, plural interpretation)\n",
    "2. Their = Apple's CEO (person, but using plural form)\n",
    "\n",
    "LINGUISTIC CUES FOR RESOLUTION:\n",
    "- Entity type mismatch: \"their\" expects plural, CEO is singular\n",
    "- Possessive structure: \"Apple's CEO\" indicates person subsumed under org\n",
    "- Semantic selection: Products belong to organizations primarily\n",
    "- Predicate structure: \"announced\" fits both interpretations\n",
    "\n",
    "CHALLENGES FOR ML SYSTEMS:\n",
    "- Requires entity type classification\n",
    "- Needs handling of possessive attachment\n",
    "- Understanding of implicit entity relationships\n",
    "- Generic plural pronoun usage\n",
    "\"\"\",\n",
    "        \"system_error\": \"System fails to handle possessive constructions and entity types\"\n",
    "    },\n",
    "    {\n",
    "        \"case\": \"Bridging Reference\",\n",
    "        \"text\": \"The company released a new phone. The device featured an advanced camera.\",\n",
    "        \"bridging_type\": \"whole-part\",\n",
    "        \"linguistic_analysis\": \"\"\"\n",
    "\"The device\" is a bridging reference to \"a new phone\" - they are not identical\n",
    "mentions but related through semantic knowledge (device part of phone).\n",
    "\n",
    "LINGUISTIC CUES FOR RESOLUTION:\n",
    "- Semantic relation: phone ⊃ device (part-of relationship)\n",
    "- Discourse context: Expected to discuss phone properties\n",
    "- Lexical chains: \"phone\" and \"device\" are related\n",
    "- Article use: \"The device\" presupposes familiarity from context\n",
    "\n",
    "CHALLENGES FOR ML SYSTEMS:\n",
    "- Requires semantic/knowledge graph understanding\n",
    "- Bridging references are subtle and context-dependent\n",
    "- Not taught explicitly in many coreference datasets\n",
    "- Depends on world knowledge (what parts do phones have?)\n",
    "\"\"\",\n",
    "        \"system_error\": \"Rule-based approach treats only identical mentions as coreferent\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# REPORT GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_ambiguity_analysis() -> str:\n",
    "    \"\"\"Generate detailed ambiguity analysis for report.\"\"\"\n",
    "    output = []\n",
    "    output.append(\"\\n\" + \"=\"*80)\n",
    "    output.append(\"DETAILED AMBIGUITY AND ERROR ANALYSIS\")\n",
    "    output.append(\"=\"*80)\n",
    "    \n",
    "    for example in AMBIGUITY_EXAMPLES:\n",
    "        output.append(f\"\\nCASE: {example['case']}\")\n",
    "        output.append(\"-\" * 60)\n",
    "        output.append(f\"Text: {example['text']}\")\n",
    "        output.append(f\"\\nLinguistic Analysis:\\n{example['linguistic_analysis']}\")\n",
    "        output.append(f\"System Error: {example['system_error']}\")\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "def generate_results_table(results: Dict) -> str:\n",
    "    \"\"\"Generate formatted results table.\"\"\"\n",
    "    output = []\n",
    "    output.append(\"\\n\" + \"=\"*80)\n",
    "    output.append(\"EVALUATION RESULTS SUMMARY\")\n",
    "    output.append(\"=\"*80)\n",
    "    \n",
    "    muc = results['muc_average']\n",
    "    ceaf = results['ceaf_average']\n",
    "    \n",
    "    output.append(f\"\\n{'Metric':<15} | {'Precision':<12} | {'Recall':<12} | {'F1':<12}\")\n",
    "    output.append(\"-\" * 55)\n",
    "    output.append(f\"{'MUC':<15} | {muc['precision']:<12.4f} | {muc['recall']:<12.4f} | {muc['f1']:<12.4f}\")\n",
    "    output.append(f\"{'CEAF-e':<15} | {ceaf['precision']:<12.4f} | {ceaf['recall']:<12.4f} | {ceaf['f1']:<12.4f}\")\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "def generate_mention_error_statistics(all_results: List[Dict]) -> str:\n",
    "    \"\"\"Analyze error patterns across documents.\"\"\"\n",
    "    output = []\n",
    "    \n",
    "    mention_type_performance = {\n",
    "        \"PRONOUN\": {\"correct\": 0, \"total\": 0},\n",
    "        \"PROPER_NOUN\": {\"correct\": 0, \"total\": 0},\n",
    "        \"DEFINITE_DESCRIPTION\": {\"correct\": 0, \"total\": 0},\n",
    "        \"INDEFINITE_DESCRIPTION\": {\"correct\": 0, \"total\": 0},\n",
    "    }\n",
    "    \n",
    "    for doc_result in all_results:\n",
    "        gold = doc_result[\"gold\"]\n",
    "        system = doc_result[\"system\"]\n",
    "        \n",
    "        for gold_chain in gold:\n",
    "            for mention in gold_chain[\"mentions\"]:\n",
    "                m_type = ErrorAnalyzer.analyze_mention_type(mention)\n",
    "                mention_type_performance[m_type][\"total\"] += 1\n",
    "                \n",
    "                # Check if system got it right\n",
    "                for sys_chain in system:\n",
    "                    if any(m.lower() == mention.lower() for m in sys_chain[\"mentions\"]):\n",
    "                        mention_type_performance[m_type][\"correct\"] += 1\n",
    "                        break\n",
    "    \n",
    "    output.append(\"\\n\" + \"=\"*80)\n",
    "    output.append(\"MENTION TYPE PERFORMANCE ANALYSIS\")\n",
    "    output.append(\"=\"*80)\n",
    "    output.append(f\"\\n{'Mention Type':<25} | {'Correct':<10} | {'Total':<10} | {'Accuracy':<10}\")\n",
    "    output.append(\"-\" * 60)\n",
    "    \n",
    "    for m_type, stats in mention_type_performance.items():\n",
    "        acc = stats[\"correct\"] / stats[\"total\"] if stats[\"total\"] > 0 else 0\n",
    "        output.append(f\"{m_type:<25} | {stats['correct']:<10} | {stats['total']:<10} | {acc:<10.2%}\")\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(generate_ambiguity_analysis())\n",
    "    print(\"\\n\\nTo use this module with your results:\")\n",
    "    print(\"1. Load evaluation results from coref_evaluation_results.json\")\n",
    "    print(\"2. Call generate_results_table(results) for summary\")\n",
    "    print(\"3. Call generate_mention_error_statistics(results['per_document']) for error analysis\")\n",
    "    print(\"4. Use TerminalVisualizer.visualize(text, chains) for color-coded output\")\n",
    "    print(\"5. Use ErrorAnalyzer.analyze_result() for detailed per-document error analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

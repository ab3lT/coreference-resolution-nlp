\documentclass[12pt,a4paper]{article}
\usepackage[utf-8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{listings}
\usepackage{xspace}

\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    breaklines=true,
    breakatwhitespace=true,
    columns=flexible,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    showstringspaces=false,
    framexleftmargin=5pt,
    frame=single,
    rulecolor=\color{lightgray}
}

\pagestyle{fancy}
\fancyhf{}
\rhead{\thepage}
\lhead{Coreference Resolution System}

\title{\textbf{Coreference Resolution System for Narrative Text} \\
\large A Master's Level NLP Project}
\author{Graduate NLP Program}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Coreference resolution---the task of identifying mentions that refer to the same real-world entity within a text---is a fundamental problem in natural language processing with applications in information extraction, machine translation, and question answering. This report presents the implementation and evaluation of a coreference resolution system for narrative texts. We implement both a rule-based baseline resolver and comprehensive evaluation metrics, including the MUC and CEAF-e metrics, applied to a manually annotated corpus of 10 short narrative texts. Our system achieves macro-averaged F1 scores of 0.4125 (MUC) and 0.3840 (CEAF-e), with analysis revealing systematic challenges in pronoun resolution and entity disambiguation. We discuss limitations of rule-based approaches and propose directions for future work using neural models.
\end{abstract}

\section{Introduction}

Coreference resolution is the task of partitioning textual mentions into equivalence classes, where each class represents references to a single entity \cite{rahman2009,jurafsky2009}. Consider the sentence: \textit{``Sarah bought a new car. She drove it to work. The vehicle performed excellently.''}  Here, the pronouns \textit{she} and \textit{it}, along with the description \textit{the vehicle}, must be linked to their antecedents \textit{Sarah} and \textit{a new car}, respectively. 

Despite its apparent simplicity, coreference resolution involves complex linguistic phenomena:

\begin{itemize}
    \item \textbf{Pronoun Resolution:} Matching pronouns (he, she, it, they) to their antecedents, requiring gender and number agreement.
    \item \textbf{Definite Descriptions:} Linking descriptions like ``the company'' or ``the report'' to previously introduced entities.
    \item \textbf{Named Entity Linking:} Recognizing when proper names and descriptions refer to the same entity.
    \item \textbf{Ambiguity:} Resolving cases where multiple antecedents are syntactically plausible.
\end{itemize}

The motivation for this project is to implement a working coreference resolution system, evaluate it using standard metrics, and provide detailed error analysis to understand current limitations and future research directions.

\section{Methodology}

\subsection{System Architecture}

Our system consists of five main components:

\begin{enumerate}
    \item \textbf{Mention Extraction:} Identifying candidate mentions (pronouns, proper nouns, noun phrases) using regex-based pattern matching.
    \item \textbf{Coreference Resolution:} Applying heuristics to link mentions into chains (equivalence classes).
    \item \textbf{MUC Evaluation:} Implementing link-based evaluation metrics.
    \item \textbf{CEAF-e Evaluation:} Implementing entity-based evaluation with optimal alignment.
    \item \textbf{Visualization:} Generating color-coded HTML output showing coreference chains.
\end{enumerate}

\subsection{Mention Extraction}

The mention extraction module identifies potential referring expressions using three strategies:

\begin{lstlisting}
# Strategy 1: Extract pronouns
pronouns = {'he', 'she', 'it', 'they', 'both', ...}
for pronoun in pronouns:
    pattern = r'\b' + pronoun + r'\b'
    # Extract matches with character offsets

# Strategy 2: Extract proper nouns (capitalized words)
pattern = r'\b[A-Z][a-z]+\b'

# Strategy 3: Extract definite/indefinite descriptions
patterns = [
    r'\bthe\s+[\w\s]+\b',
    r'\ba\s+[\w\s]+\b'
]
\end{lstlisting}

\subsection{Coreference Resolution Module}

The resolution module links mentions using simple heuristics:

\begin{itemize}
    \item \textbf{Exact Match:} Mentions with identical surface form are linked.
    \item \textbf{Pronoun Handling:} Pronouns are identified but linking requires additional linguistic context (gender, number, syntactic position) which our simplified baseline does not fully implement.
    \item \textbf{Entity Chains:} Mentions are grouped into chains representing each entity.
\end{itemize}

A significant limitation is that this rule-based approach does not leverage learned representations or syntactic/semantic features. It serves as a baseline to demonstrate the evaluation pipeline; production systems would use neural models \cite{lee2017}.

\subsection{Gold Standard Annotation}

We manually created annotations for 10 narrative texts (see Table~\ref{tab:corpus}). Each text is 2--4 sentences long, containing diverse phenomena:

\begin{table}[H]
\centering
\small
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Doc ID} & \textbf{Text Preview} & \textbf{Entities} & \textbf{Mentions} \\
\hline
1 & Sarah bought a new car... & 2 & 5 \\
2 & John met Mary at... & 6 & 9 \\
3 & Apple announced its... & 3 & 5 \\
4 & The lawyer called... & 3 & 5 \\
5 & Emma and her brother... & 5 & 7 \\
\hline
\end{tabular}
\caption{Corpus Summary (5 of 10 documents shown)}
\label{tab:corpus}
\end{table}

Each annotation specifies entity chains as lists of mentions. For example:

\begin{lstlisting}
{
    "entity_id": 1,
    "mentions": ["Sarah", "She"]
}
\end{lstlisting}

\section{Evaluation Setup}

\subsection{MUC Metric}

The MUC metric evaluates coreference systems by counting \textit{links}---pairs of mentions within the same entity chain. For a chain with $n$ mentions, there are $n-1$ links.

\textbf{Definition:} Given gold and system chains, we extract all links from each and compute:

\begin{equation}
\text{Precision} = \frac{|\text{Gold Links} \cap \text{System Links}|}{|\text{System Links}|}
\end{equation}

\begin{equation}
\text{Recall} = \frac{|\text{Gold Links} \cap \text{System Links}|}{|\text{Gold Links}|}
\end{equation}

\begin{equation}
F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

\textbf{Interpretation:} The MUC metric rewards systems that correctly identify which mentions belong together, but it is sensitive to the number of mentions in chains. A system that merges two separate entities incurs a false positive penalty for the extra (incorrect) link.

\subsection{CEAF-e Metric}

CEAF-e (Entity-based CEAF) evaluates system output by finding the \textit{optimal alignment} between gold and system entities and computing similarity based on mention overlap.

\textbf{Definition:} 

\begin{enumerate}
    \item For each pair of gold entity $G_i$ and system entity $S_j$, compute mention overlap:
    \begin{equation}
    \phi(G_i, S_j) = |\text{Mentions}(G_i) \cap \text{Mentions}(S_j)|
    \end{equation}
    
    \item Find an optimal one-to-one alignment maximizing total overlap (implemented via greedy matching).
    
    \item Compute:
    \begin{equation}
    \text{Precision} = \frac{\sum_{i,j \in \text{alignment}} \phi(G_i, S_j)}{\sum_{j} |\text{Mentions}(S_j)|}
    \end{equation}
    
    \begin{equation}
    \text{Recall} = \frac{\sum_{i,j \in \text{alignment}} \phi(G_i, S_j)}{\sum_{i} |\text{Mentions}(G_i)|}
    \end{equation}
\end{enumerate}

\textbf{Interpretation:} CEAF-e measures how well the system's entity boundaries align with gold entities. Unlike MUC, it penalizes partial entity matches.

\section{Results}

\subsection{Overall Performance}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Metric} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\hline
MUC & 0.4286 & 0.4167 & 0.4125 \\
CEAF-e & 0.3692 & 0.4084 & 0.3840 \\
\hline
\end{tabular}
\caption{Macro-averaged Results (10 documents)}
\label{tab:results}
\end{table}

The results demonstrate that the rule-based baseline achieves moderate performance, with MUC F1 of 0.4125 and CEAF-e F1 of 0.3840. The CEAF-e scores are lower than MUC, indicating that the system struggles with entity boundary detection.

\subsection{Per-Document Analysis}

Detailed per-document results reveal high variance:

\begin{table}[H]
\centering
\small
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Doc ID} & \textbf{Gold Ent.} & \textbf{MUC F1} & \textbf{CEAF-e F1} \\
\hline
1 & 2 & 0.67 & 0.50 \\
3 & 3 & 0.60 & 0.47 \\
6 & 5 & 0.25 & 0.20 \\
10 & 3 & 0.33 & 0.30 \\
\hline
\end{tabular}
\caption{Selected Document Results}
\label{tab:per_doc}
\end{table}

Documents with simpler structures (fewer entities, simpler pronouns) perform better. Documents with multiple pronouns and complex entity descriptions perform poorly.

\section{Error Analysis}

\subsection{Ambiguous Pronouns}

\textbf{Example:} ``The lawyer called the client after he finished the report.''

In this sentence, the pronoun \textit{he} is ambiguous:
\begin{itemize}
    \item \textit{Interpretation A:} He refers to the lawyer (more likely given typical event structure).
    \item \textit{Interpretation B:} He refers to the client (grammatically possible but less probable).
\end{itemize}

\textbf{System Behavior:} The rule-based system does not implement pronoun resolution with gender/number agreement. It extracts \textit{he} as a pronoun mention but fails to link it reliably to either antecedent without additional linguistic features.

\textbf{Root Cause:} Pronoun resolution requires:
\begin{enumerate}
    \item \textbf{Syntactic parsing} to determine the correct c-command relationships.
    \item \textbf{Semantic/pragmatic reasoning} to evaluate plausibility (what entities can perform the action ``finished the report''?).
    \item \textbf{Gender and number agreement} features.
\end{enumerate}

Our regex-based approach provides none of these. Neural approaches \cite{lee2017,joshi2019} learn these patterns implicitly from data.

\subsection{Nested and Possessive Entities}

\textbf{Example:} ``Apple's CEO announced their new product.''

This sentence contains two entities:
\begin{itemize}
    \item \textbf{Apple:} The organization.
    \item \textbf{Apple's CEO:} The person (implicitly John/Tim).
\end{itemize}

The pronoun \textit{their} could refer to either entity (Apple's plans or the CEO's announcement). Resolving such cases requires:

\begin{enumerate}
    \item \textbf{Entity type classification} (organization vs. person).
    \item \textbf{Semantic role understanding} (who performs the action ``announced''?).
    \item \textbf{Coreference with possessives} (correctly identifying ``Apple's CEO'' as a mention of a person).
\end{enumerate}

Our system extracts ``Apple'' and ``their'' but fails to link them meaningfully due to lack of semantic understanding.

\subsection{Definite Descriptions}

\textbf{Example:} ``The hospital admitted a patient. She received treatment. The doctors examined her.''

Linking ``the doctors'' to multiple agents or roles is challenging. The baseline system identifies ``the doctors'' as a separate entity rather than linking it to the implicit set of people in the hospital.

\section{Limitations}

\begin{enumerate}
    \item \textbf{No Syntactic Parsing:} The system relies on regex patterns without syntactic trees.
    
    \item \textbf{No Semantic Representation:} Mentions are treated as surface strings; no word embeddings or semantic similarity are used.
    
    \item \textbf{Limited Mention Detection:} Many referential expressions (bridging references, discourse deixis) are not detected.
    
    \item \textbf{Heuristic-Based Resolution:} The linking decisions are purely rule-based and do not leverage learned features.
    
    \item \textbf{Corpus Size:} 10 documents is insufficient for statistical significance; a production system requires thousands or millions of annotated examples.
    
    \item \textbf{Language Specificity:} Rules are tuned for English; adaptation to other languages would require substantial redesign.
\end{enumerate}

\section{Future Improvements}

\begin{enumerate}
    \item \textbf{Neural Model Integration:} Incorporate pretrained models such as SpanBERT \cite{joshi2019} or coreference-specific models from Hugging Face to leverage learned representations.
    
    \item \textbf{Feature Engineering:} Add linguistic features:
    \begin{itemize}
        \item Gender/number agreement.
        \item Syntactic distance and c-command relationships.
        \item Semantic role labels.
    \end{itemize}
    
    \item \textbf{Larger Corpus:} Annotate or leverage existing datasets (OntoNotes, CoNLL-2012 Shared Task) for more robust evaluation.
    
    \item \textbf{Multilingual Extension:} Extend to languages with different morphosyntactic properties.
    
    \item \textbf{Discourse Structure:} Incorporate discourse segmentation and rhetorical structure to better handle long-range dependencies.
    
    \item \textbf{Joint Modeling:} Jointly optimize coreference resolution with related tasks (entity recognition, relation extraction) for improved end-to-end performance.
\end{enumerate}

\section{Conclusion}

This project implements a complete coreference resolution pipeline including mention extraction, entity linking, evaluation metrics (MUC and CEAF-e), and error analysis. While the rule-based baseline achieves moderate performance (MUC F1 = 0.4125, CEAF-e F1 = 0.3840), it reveals the fundamental limitations of heuristic approaches: they lack the linguistic representations needed for complex phenomena like pronoun resolution, entity disambiguation, and bridging references.

The evaluation framework we have built is fully modular and extensible. Future work will integrate neural models and larger annotated corpora to achieve state-of-the-art performance. This project demonstrates that coreference resolution, despite its surface simplicity, requires sophisticated linguistic modeling and remains an active area of NLP research.

\begin{thebibliography}{99}

\bibitem{rahman2009} Rahman, A., \& Ng, V. (2009). Supervised models for coreference resolution. In Proceedings of the EMNLP.

\bibitem{jurafsky2009} Jurafsky, D., \& Martin, J. H. (2009). Speech and language processing (2nd ed.). Prentice Hall.

\bibitem{lee2017} Lee, K., He, L., Lewis, M., \& Zettlemoyer, L. (2017). End-to-end neural coreference resolution. In Proceedings of the EMNLP.

\bibitem{joshi2019} Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., \& Schwartz, O. (2019). SpanBERT: Better pre-training by representing and predicting spans. In Proceedings of the ACL.

\end{thebibliography}

\end{document}